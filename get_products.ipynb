{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "from seleniumwire import webdriver\n",
    "from selenium.webdriver.chrome.options import Options\n",
    "from selenium.webdriver.chrome.service import Service\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver import ActionChains\n",
    "from selenium.webdriver.support.ui import WebDriverWait\n",
    "from selenium.webdriver.support import expected_conditions as EC\n",
    "from webdriver_manager.chrome import ChromeDriverManager\n",
    "from lxml import etree\n",
    "from bs4 import BeautifulSoup\n",
    "from urllib.parse import urlparse\n",
    "import re\n",
    "from pymongo import MongoClient\n",
    "from datetime import datetime\n",
    "import json\n",
    "import os\n",
    "\n",
    "from functions.getProxy import *\n",
    "from functions.getUserAgent import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "#proxy = getProxy()\n",
    "domain = 'shein.com' # For checking if the URL is from the same domain\n",
    "debug = True # Set to True to limit to 1 page\n",
    "db_mode = False # True = MongoDB, False = JSON"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('shein_categories.txt', 'r') as file: # Read URLs from file\n",
    "    urls = file.readlines()\n",
    "\n",
    "if db_mode:\n",
    "    mongo_host = os.environ.get('MONGO_HOST', 'localhost')\n",
    "    client = MongoClient(f'mongodb://{mongo_host}:27017/')\n",
    "    db = client['shein']\n",
    "    collection = db['product_urls']\n",
    "\n",
    "    # Setup Index\n",
    "    try:\n",
    "        collection.create_index('url', unique=True)\n",
    "    except Exception as e:\n",
    "        print('Index already exists')\n",
    "\n",
    "blacklistedWords = [\n",
    "    'javascript:',\n",
    "    'mailto:',\n",
    "    'tel:',\n",
    "    'facebook.com',\n",
    "    'twitter.com',\n",
    "    'instagram.com',\n",
    "    'youtube.com',\n",
    "    'pinterest.com',\n",
    "    'tiktok.com',\n",
    "    'Copyright',\n",
    "    'copyright',\n",
    "    'Privacy',\n",
    "    'privacy',\n",
    "    'Terms',\n",
    "    'terms',\n",
    "    'Imprint',\n",
    "    'imprint',\n",
    "    'bonus',\n",
    "    'campaign',\n",
    "    'campaigns',\n",
    "    'sale',\n",
    "    'refund',\n",
    "    'track',\n",
    "    'How-to',\n",
    "    'how-to',\n",
    "    'shein.com/women',\n",
    "    'shein.com/other',\n",
    "    'shein.com/Return-Policy',\n",
    "    'shein.com/men',\n",
    "    'shein.com/plussize',\n",
    "    'shein.com/curve-plus-size',\n",
    "    'promotion',\n",
    "    'shein.com/home',\n",
    "    'shein.com/cart',\n",
    "    'contact',\n",
    "    'About',\n",
    "    'SUPPLY-CHAIN-TRANSPARENCY',\n",
    "    'prime',\n",
    "    'shein.com/kids',\n",
    "    'shein.com/beauty',\n",
    "    'shein.com/flashsale',\n",
    "    'Shipping-Info',\n",
    "    'coupon-a',\n",
    "    '/user/auth/login',\n",
    "    'daily-new',\n",
    "    'New-in-Trends',\n",
    "    'shein.com/style',\n",
    "    'New-in-Trends',\n",
    "    'shein.com/member-image-list',\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to check if any word from a list is included in a string\n",
    "def included_in_string(string, word_list):\n",
    "    for word in word_list:\n",
    "        if word in string:\n",
    "            return True\n",
    "    return False\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing https://us.shein.com/Plus-Size-Blouses-c-1891.html\n",
      "Error getting pagination: Message: \n",
      "Stacktrace:\n",
      "\tGetHandleVerifier [0x00007FF6C3A06CF5+28821]\n",
      "\t(No symbol) [0x00007FF6C3973880]\n",
      "\t(No symbol) [0x00007FF6C381578A]\n",
      "\t(No symbol) [0x00007FF6C38691BE]\n",
      "\t(No symbol) [0x00007FF6C38694AC]\n",
      "\t(No symbol) [0x00007FF6C38B2647]\n",
      "\t(No symbol) [0x00007FF6C388F33F]\n",
      "\t(No symbol) [0x00007FF6C38AF412]\n",
      "\t(No symbol) [0x00007FF6C388F0A3]\n",
      "\t(No symbol) [0x00007FF6C385A778]\n",
      "\t(No symbol) [0x00007FF6C385B8E1]\n",
      "\tGetHandleVerifier [0x00007FF6C3D3FCED+3408013]\n",
      "\tGetHandleVerifier [0x00007FF6C3D5745F+3504127]\n",
      "\tGetHandleVerifier [0x00007FF6C3D4B63D+3455453]\n",
      "\tGetHandleVerifier [0x00007FF6C3ACBDFB+835995]\n",
      "\t(No symbol) [0x00007FF6C397EB9F]\n",
      "\t(No symbol) [0x00007FF6C397A854]\n",
      "\t(No symbol) [0x00007FF6C397A9ED]\n",
      "\t(No symbol) [0x00007FF6C396A1D9]\n",
      "\tBaseThreadInitThunk [0x00007FFD1AED259D+29]\n",
      "\tRtlUserThreadStart [0x00007FFD1C48AF38+40]\n",
      "\n",
      "Processing page 1 of 1\n",
      "Writing to JSON file\n"
     ]
    }
   ],
   "source": [
    "options = Options()\n",
    "options.add_argument('--headless')\n",
    "options.add_argument('--no-sandbox')\n",
    "options.add_argument('--disable-dev-shm-usage')\n",
    "options.add_argument('--user-agent=' + GET_UA())\n",
    "options.add_argument('--incognito')\n",
    "options.binary_location = \"C:\\\\Program Files\\\\Google\\\\Chrome\\\\Application\\\\chrome.exe\"\n",
    "chrome_drvier_binary = \"C:\\\\chromedriver-win64\\\\chromedriver.exe\"\n",
    "\n",
    "driver = webdriver.Chrome(service=Service(chrome_drvier_binary), options=options)\n",
    "\n",
    "for url in urls:\n",
    "    url = url.strip()\n",
    "    print('Processing ' + url)\n",
    "\n",
    "    driver.get(url)\n",
    "\n",
    "    try:\n",
    "        # Use WebDriverWait for better handling\n",
    "        pagination_text = WebDriverWait(driver, 10).until(\n",
    "            EC.presence_of_element_located((By.CLASS_NAME, 'sui-pagination__total'))\n",
    "        ).text\n",
    "        pagination_number = re.sub(r\"\\D\", \"\", pagination_text)  # Fix: Raw string\n",
    "        max_pages = int(pagination_number)\n",
    "        if debug:\n",
    "            max_pages = min(1, max_pages)\n",
    "        print(f'Found {max_pages} pages')\n",
    "    except Exception as e:\n",
    "        print('Error getting pagination: ' + str(e))\n",
    "        max_pages = 1  # Default to 1 page if pagination fails\n",
    "\n",
    "    # Initialize array for product URLs\n",
    "    product_urls = []\n",
    "\n",
    "    for i in range(1, max_pages + 1):\n",
    "        try:\n",
    "            print(f'Processing page {i} of {max_pages}')\n",
    "            driver.get(url + '?page=' + str(i))\n",
    "\n",
    "            product_elements = driver.find_elements(By.CLASS_NAME, 'product-list__item')\n",
    "            for product in product_elements:\n",
    "                href = product.find_element(By.TAG_NAME, 'a').get_attribute('href')\n",
    "                if not href or included_in_string(href, blacklistedWords):\n",
    "                    continue\n",
    "                parsed_url = urlparse(href)\n",
    "                cleaned_url = parsed_url.scheme + \"://\" + parsed_url.netloc + parsed_url.path\n",
    "                if domain in parsed_url.netloc and cleaned_url not in product_urls:\n",
    "                    try:\n",
    "                        if db_mode:\n",
    "                            if collection.find_one({'url': cleaned_url}):\n",
    "                                print('URL already exists in MongoDB')\n",
    "                                continue\n",
    "                            print('Adding ' + cleaned_url + ' to MongoDB')\n",
    "                            collection.insert_one({'url': cleaned_url, 'status': 'pending', 'timestamp': datetime.now()})\n",
    "                        else:\n",
    "                            product_urls.append(cleaned_url)\n",
    "                    except Exception as e:\n",
    "                        print('Error adding URL to MongoDB: ' + str(e))\n",
    "                        pass\n",
    "        except Exception as e:\n",
    "            print('Error processing page: ' + str(e))\n",
    "            continue\n",
    "\n",
    "    if not db_mode:\n",
    "        print('Writing to JSON file')\n",
    "        with open('product_urls.json', 'w') as outfile:\n",
    "            json.dump(product_urls, outfile)\n",
    "\n",
    "driver.quit()\n",
    "if db_mode:\n",
    "    client.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing the first page only (max_pages set to 1)\n",
      "Processing page 1 of 1\n",
      "Error processing page: HTTPConnectionPool(host='localhost', port=53256): Max retries exceeded with url: /session/40a0337a7d1afa82df76ac653cee1502/url (Caused by NewConnectionError('<urllib3.connection.HTTPConnection object at 0x00000218FF4A11C0>: Failed to establish a new connection: [WinError 10061] No connection could be made because the target machine actively refused it'))\n",
      "Writing to JSON file\n"
     ]
    }
   ],
   "source": [
    "# Skip pagination check and directly set max_pages to 1\n",
    "max_pages = 1\n",
    "print(f'Processing the first page only (max_pages set to {max_pages})')\n",
    "\n",
    "# Initialize array for product URLs\n",
    "product_urls = []\n",
    "\n",
    "for i in range(1, max_pages + 1):\n",
    "    try:\n",
    "        print(f'Processing page {i} of {max_pages}')\n",
    "        driver.get(url + f'?page={i}')\n",
    "\n",
    "        product_elements = driver.find_elements(By.CLASS_NAME, 'product-list__item')\n",
    "        for product in product_elements:\n",
    "            href = product.find_element(By.TAG_NAME, 'a').get_attribute('href')\n",
    "            if not href or included_in_string(href, blacklistedWords):\n",
    "                continue\n",
    "            parsed_url = urlparse(href)\n",
    "            cleaned_url = parsed_url.scheme + \"://\" + parsed_url.netloc + parsed_url.path\n",
    "            if domain in parsed_url.netloc and cleaned_url not in product_urls:\n",
    "                try:\n",
    "                    if db_mode:\n",
    "                        if collection.find_one({'url': cleaned_url}):\n",
    "                            print('URL already exists in MongoDB')\n",
    "                            continue\n",
    "                        print('Adding ' + cleaned_url + ' to MongoDB')\n",
    "                        collection.insert_one({'url': cleaned_url, 'status': 'pending', 'timestamp': datetime.now()})\n",
    "                    else:\n",
    "                        product_urls.append(cleaned_url)\n",
    "                except Exception as e:\n",
    "                    print('Error adding URL to MongoDB: ' + str(e))\n",
    "                    pass\n",
    "    except Exception as e:\n",
    "        print('Error processing page: ' + str(e))\n",
    "        continue\n",
    "\n",
    "if not db_mode:\n",
    "    print('Writing to JSON file')\n",
    "    with open('product_urls.json', 'w') as outfile:\n",
    "        json.dump(product_urls, outfile)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
